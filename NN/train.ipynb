{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/menglai/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/menglai/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/menglai/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/menglai/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/menglai/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/menglai/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision as tv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model import CorrResNet18\n",
    "# from dataset import imgcontroldataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'  # only has 1 GPU set GPU 0 as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "batch_size = 27\n",
    "learning_rate = 0.001\n",
    "arch = 'resnet18'\n",
    "# load the pre-trained weights\n",
    "model_file = './pre_trained/%s_places365.pth.tar' % arch\n",
    "writer = SummaryWriter('./runs/exp1_129')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrResNet18(nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions = 4, fine_tuning = False):\n",
    "        super(CorrResNet18, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=False)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = fine_tuning\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool   #  output feature dimension 512 for input image dimension 224*224\n",
    "        self.num_ftrs = resnet.fc.in_features\n",
    "        self.linear_sensor = nn.Linear(5, 128)\n",
    "        self.linear_final = nn.Linear(2*self.num_ftrs+2*128, num_actions)       \n",
    "\n",
    "    def forward(self, img, meta): # image pairs are cat along the channel dimension [batch, 6, width, height]\n",
    "        siam1 = []\n",
    "        siam2 = []\n",
    "        for i in range(2): # the siamese network architecture \n",
    "            # for image\n",
    "            x = self.relu(self.bn1(self.conv1(img[:,(i*3):(i+1)*3, :, :])))\n",
    "            x = self.maxpool(x)\n",
    "            x = self.layer4(self.layer3(self.layer2(self.layer1(x))))\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(-1, self.num_ftrs)\n",
    "            siam1.append(x)\n",
    "            # for sensor data\n",
    "            y = self.linear_sensor(meta[:,:,i])\n",
    "            siam2.append(y)\n",
    "        out = torch.cat((siam1[0], siam1[1], siam2[0], siam2[1]), dim = 1)\n",
    "        out = self.linear_final(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CorrResNet18() #models.__dict__[arch](num_classes=365)\n",
    "checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
    "del state_dict['fc.weight']\n",
    "del state_dict['fc.bias']\n",
    "state_dict[\"linear_sensor.weight\"] = nn.init.xavier_uniform_(model.linear_sensor.weight.data).float()\n",
    "state_dict[\"linear_sensor.bias\"] = model.linear_sensor.bias.data.fill_(0.01).float()\n",
    "state_dict[\"linear_final.weight\"] = nn.init.xavier_uniform_(model.linear_final.weight.data).float()\n",
    "state_dict[\"linear_final.bias\"] = model.linear_final.bias.data.fill_(0.01).float()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imgcontroldataset(data.Dataset):\n",
    "    def __init__(self, txt_dir = '.', mode = 'train', image_size = (224, 224)):\n",
    "        super(imgcontroldataset, self).__init__()\n",
    "        self.datacsv = np.loadtxt(os.path.join(txt_dir, \"%s.txt\" % mode)).astype(int)\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 3*self.datacsv.shape[0]  # augment the dataset for 3 times, randomness from randomcrop\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.datacsv)\n",
    "        \n",
    "        img_cur_path = os.path.join('./train_data/image',str(self.datacsv[idx, 0]),str(self.datacsv[idx, 1]),str(self.datacsv[idx, 2])+'error.png')\n",
    "        img_cur = Image.open(img_cur_path).convert('RGB')\n",
    "        img_exp_path = os.path.join('./train_data/image',str(self.datacsv[idx, 0]),str(self.datacsv[idx, 1]),str(self.datacsv[idx, 2])+'after_cor.png')\n",
    "        img_exp = Image.open(img_exp_path).convert('RGB')  \n",
    "        meta_cur_path = os.path.join('./train_data/metadata',str(self.datacsv[idx, 0]),str(self.datacsv[idx, 1]),str(self.datacsv[idx, 2])+'error.txt')\n",
    "        meta_cur = np.loadtxt(meta_cur_path)\n",
    "        meta_exp_path = os.path.join('./train_data/metadata',str(self.datacsv[idx, 0]),str(self.datacsv[idx, 1]),str(self.datacsv[idx, 2])+'after_cor.txt')\n",
    "        meta_exp = np.loadtxt(meta_exp_path)\n",
    "        ctrl_path = os.path.join('./train_data/ctrl',str(self.datacsv[idx, 0]),str(self.datacsv[idx, 1]),str(self.datacsv[idx, 2])+'converted_nml.txt')\n",
    "        ctrl = np.loadtxt(ctrl_path)\n",
    "        \n",
    "        # img transformation\n",
    "        transform = tv.transforms.Compose([        \n",
    "            tv.transforms.Resize(min(self.image_size)),    # fix the ratio and keep the smaller edge to 224 according to Relative **\n",
    "            tv.transforms.RandomCrop(self.image_size),\n",
    "            tv.transforms.ColorJitter(),  # randomly change hue, contrast illumination\n",
    "            tv.transforms.ToTensor(),    \n",
    "            tv.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),    # copied from pytorch tutorial imagenet\n",
    "            ])\n",
    "        img1 = transform(img_exp)\n",
    "        img2 = transform(img_cur)\n",
    "        img = torch.cat((img1, img2), dim = 0).float()  # The dimension 0 is the RGB channel\n",
    "        meta = torch.cat((torch.from_numpy(meta_exp)[:, None], torch.from_numpy(meta_cur)[:, None]), dim = 1).float()\n",
    "        ctrl = torch.from_numpy(ctrl).float()\n",
    "        \n",
    "        return img, meta, ctrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_set = imgcontroldataset()\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, pin_memory=False)\n",
    "val_set = imgcontroldataset(mode=\"val\")\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=False, pin_memory=False)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [16/17] Loss: 0.0834\n",
      "Epoch [2/80], Step [16/17] Loss: 0.0694\n",
      "Epoch [3/80], Step [16/17] Loss: 0.0673\n",
      "Epoch [4/80], Step [16/17] Loss: 0.0746\n",
      "Epoch [5/80], Step [16/17] Loss: 0.0520\n",
      "Epoch [6/80], Step [16/17] Loss: 0.0429\n",
      "Epoch [7/80], Step [16/17] Loss: 0.0464\n",
      "Epoch [8/80], Step [16/17] Loss: 0.0373\n",
      "Epoch [9/80], Step [16/17] Loss: 0.0455\n",
      "Epoch [10/80], Step [16/17] Loss: 0.0427\n",
      "Epoch [11/80], Step [16/17] Loss: 0.0409\n",
      "Epoch [12/80], Step [16/17] Loss: 0.0467\n",
      "Epoch [13/80], Step [16/17] Loss: 0.0502\n",
      "Epoch [14/80], Step [16/17] Loss: 0.0381\n",
      "Epoch [15/80], Step [16/17] Loss: 0.0357\n",
      "Epoch [16/80], Step [16/17] Loss: 0.0309\n",
      "Epoch [17/80], Step [16/17] Loss: 0.0242\n",
      "Epoch [18/80], Step [16/17] Loss: 0.0316\n",
      "Epoch [19/80], Step [16/17] Loss: 0.0391\n",
      "Epoch [20/80], Step [16/17] Loss: 0.0365\n",
      "Epoch [21/80], Step [16/17] Loss: 0.0307\n",
      "Epoch [22/80], Step [16/17] Loss: 0.0233\n",
      "Epoch [23/80], Step [16/17] Loss: 0.0289\n",
      "Epoch [24/80], Step [16/17] Loss: 0.0254\n",
      "Epoch [25/80], Step [16/17] Loss: 0.0262\n",
      "Epoch [26/80], Step [16/17] Loss: 0.0284\n",
      "Epoch [27/80], Step [16/17] Loss: 0.0302\n",
      "Epoch [28/80], Step [16/17] Loss: 0.0204\n",
      "Epoch [29/80], Step [16/17] Loss: 0.0206\n",
      "Epoch [30/80], Step [16/17] Loss: 0.0237\n",
      "Epoch [31/80], Step [16/17] Loss: 0.0291\n",
      "Epoch [32/80], Step [16/17] Loss: 0.0247\n",
      "Epoch [33/80], Step [16/17] Loss: 0.0220\n",
      "Epoch [34/80], Step [16/17] Loss: 0.0235\n",
      "Epoch [35/80], Step [16/17] Loss: 0.0247\n",
      "Epoch [36/80], Step [16/17] Loss: 0.0286\n",
      "Epoch [37/80], Step [16/17] Loss: 0.0223\n",
      "Epoch [38/80], Step [16/17] Loss: 0.0214\n",
      "Epoch [39/80], Step [16/17] Loss: 0.0240\n",
      "Epoch [40/80], Step [16/17] Loss: 0.0278\n",
      "Epoch [41/80], Step [16/17] Loss: 0.0273\n",
      "Epoch [42/80], Step [16/17] Loss: 0.0157\n",
      "Epoch [43/80], Step [16/17] Loss: 0.0272\n",
      "Epoch [44/80], Step [16/17] Loss: 0.0164\n",
      "Epoch [45/80], Step [16/17] Loss: 0.0216\n",
      "Epoch [46/80], Step [16/17] Loss: 0.0229\n",
      "Epoch [47/80], Step [16/17] Loss: 0.0290\n",
      "Epoch [48/80], Step [16/17] Loss: 0.0205\n",
      "Epoch [49/80], Step [16/17] Loss: 0.0172\n",
      "Epoch [50/80], Step [16/17] Loss: 0.0273\n",
      "Epoch [51/80], Step [16/17] Loss: 0.0223\n",
      "Epoch [52/80], Step [16/17] Loss: 0.0169\n",
      "Epoch [53/80], Step [16/17] Loss: 0.0232\n",
      "Epoch [54/80], Step [16/17] Loss: 0.0242\n",
      "Epoch [55/80], Step [16/17] Loss: 0.0221\n",
      "Epoch [56/80], Step [16/17] Loss: 0.0171\n",
      "Epoch [57/80], Step [16/17] Loss: 0.0246\n",
      "Epoch [58/80], Step [16/17] Loss: 0.0180\n",
      "Epoch [59/80], Step [16/17] Loss: 0.0170\n",
      "Epoch [60/80], Step [16/17] Loss: 0.0219\n",
      "Epoch [61/80], Step [16/17] Loss: 0.0175\n",
      "Epoch [62/80], Step [16/17] Loss: 0.0202\n",
      "Epoch [63/80], Step [16/17] Loss: 0.0270\n",
      "Epoch [64/80], Step [16/17] Loss: 0.0249\n",
      "Epoch [65/80], Step [16/17] Loss: 0.0203\n",
      "Epoch [66/80], Step [16/17] Loss: 0.0196\n",
      "Epoch [67/80], Step [16/17] Loss: 0.0237\n",
      "Epoch [68/80], Step [16/17] Loss: 0.0247\n",
      "Epoch [69/80], Step [16/17] Loss: 0.0249\n",
      "Epoch [70/80], Step [16/17] Loss: 0.0211\n",
      "Epoch [71/80], Step [16/17] Loss: 0.0265\n",
      "Epoch [72/80], Step [16/17] Loss: 0.0269\n",
      "Epoch [73/80], Step [16/17] Loss: 0.0170\n",
      "Epoch [74/80], Step [16/17] Loss: 0.0241\n",
      "Epoch [75/80], Step [16/17] Loss: 0.0176\n",
      "Epoch [76/80], Step [16/17] Loss: 0.0196\n",
      "Epoch [77/80], Step [16/17] Loss: 0.0296\n",
      "Epoch [78/80], Step [16/17] Loss: 0.0197\n",
      "Epoch [79/80], Step [16/17] Loss: 0.0151\n",
      "Epoch [80/80], Step [16/17] Loss: 0.0258\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7d8d1ba0e0a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# training loop with log recorded\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "running_loss = 0.0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        image = data[0].to(device)  \n",
    "        meta = data[1].to(device)\n",
    "        labels = data[2].to(device)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(image, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 16 == 0:\n",
    "            \n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 16,\n",
    "                            epoch * len(train_loader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "#             writer.add_figure('predictions vs. actuals',\n",
    "#                             plot_classes_preds(model, (image, meta), labels),\n",
    "#                             global_step=epoch * len(train_loader) + i)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 16 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "        torch.save(model.state_dict(), '%s_model.pth' % (epoch))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in val_loader:\n",
    "        image = data[0].to(device)  \n",
    "        meta = data[1].to(device)\n",
    "        labels = data[2].to(device)\n",
    "        outputs = model(image, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "    print('MSE of the model on the test data: {} %'.format(loss))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024+256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
